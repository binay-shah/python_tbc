{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing data\n",
    "\n",
    "So far we have been learning the Python language and then learning how to use Python, the network, and databases to manipulate data.\n",
    "\n",
    "In this chapter, we take a look at three complete applications that bring all of these things together to manage and visualize data. You might use these applications as sample code to help get you started in solving a real-world problem.\n",
    "\n",
    "Each of the applications is a ZIP file that you can download and extract onto your computer and execute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Building a Google map from geocoded data\n",
    "\n",
    "In this project, we are using the Google geocoding API to clean up some user-entered geographic locations of university names and then placing the data on a Google map.\n",
    "\n",
    "\n",
    "![](../images/google-map.png)\n",
    "\n",
    "To get started, download the application\n",
    "\n",
    "geodata.zip\n",
    "\n",
    "The first problem to solve is that the free Google geocoding API is rate-limited to a certain number of requests per day. If you have a lot of data, you might need to stop and restart the lookup process several times. So we break the problem into two phases.\n",
    "\n",
    "In the first phase we take our input \"survey\" data in the file where.data and read it one line at a time, and retrieve the geocoded information from Google and store it in a database geodata.sqlite. Before we use the geocoding API for each user-entered location, we simply check to see if we already have the data for that particular line of input. The database is functioning as a local \"cache\" of our geocoding data to make sure we never ask Google for the same data twice.\n",
    "\n",
    "You can restart the process at any time by removing the file geodata.sqlite.\n",
    "\n",
    "Run the geoload.py program. This program will read the input lines in where.data and for each line check to see if it is already in the database. If we don't have the data for the location, it will call the geocoding API to retrieve the data and store it in the database.\n",
    "\n",
    "Here is a sample run after there is already some data in the database:\n",
    "\n",
    "```\n",
    "Found in database  Northeastern University\n",
    "Found in database  University of Hong Kong, ...\n",
    "Found in database  Technion\n",
    "Found in database  Viswakarma Institute, Pune, India\n",
    "Found in database  UMD\n",
    "Found in database  Tufts University\n",
    "\n",
    "Resolving Monash University\n",
    "Retrieving http://maps.googleapis.com/maps/api/\n",
    "    geocode/json?address=Monash+University\n",
    "Retrieved 2063 characters {    \"results\" : [\n",
    "{'status': 'OK', 'results': ... }\n",
    "\n",
    "Resolving Kokshetau Institute of Economics and Management\n",
    "Retrieving http://maps.googleapis.com/maps/api/\n",
    "    geocode/json?address=Kokshetau+Inst ...\n",
    "Retrieved 1749 characters {    \"results\" : [\n",
    "{'status': 'OK', 'results': ... }\n",
    "...\n",
    "```\n",
    "\n",
    "The first five locations are already in the database and so they are skipped. The program scans to the point where it finds new locations and starts retrieving them.\n",
    "\n",
    "The geoload.py program can be stopped at any time, and there is a counter that you can use to limit the number of calls to the geocoding API for each run. Given that the where.data only has a few hundred data items, you should not run into the daily rate limit, but if you had more data it might take several runs over several days to get your database to have all of the geocoded data for your input.\n",
    "\n",
    "Once you have some data loaded into geodata.sqlite, you can visualize the data using the geodump.py program. This program reads the database and writes the file where.js with the location, latitude, and longitude in the form of executable JavaScript code.\n",
    "\n",
    "A run of the geodump.py program is as follows:\n",
    "\n",
    "```\n",
    "Northeastern University, ... Boston, MA 02115, USA 42.3396998 -71.08975\n",
    "Bradley University, 1501 ... Peoria, IL 61625, USA 40.6963857 -89.6160811\n",
    "...\n",
    "Technion, Viazman 87, Kesalsaba, 32000, Israel 32.7775 35.0216667\n",
    "Monash University Clayton ... VIC 3800, Australia -37.9152113 145.134682\n",
    "Kokshetau, Kazakhstan 53.2833333 69.3833333\n",
    "...\n",
    "12 records written to where.js\n",
    "Open where.html to view the data in a browser\n",
    "```\n",
    "\n",
    "The file where.html consists of HTML and JavaScript to visualize a Google map. It reads the most recent data in where.js to get the data to be visualized. Here is the format of the where.js file:\n",
    "\n",
    "```\n",
    "myData = [\n",
    "[42.3396998,-71.08975, 'Northeastern Uni ... Boston, MA 02115'],\n",
    "[40.6963857,-89.6160811, 'Bradley University, ... Peoria, IL 61625, USA'],\n",
    "[32.7775,35.0216667, 'Technion, Viazman 87, Kesalsaba, 32000, Israel'],\n",
    "   ...\n",
    "];\n",
    "```\n",
    "\n",
    "This is a JavaScript variable that contains a list of lists. The syntax for JavaScript list constants is very similar to Python, so the syntax should be familiar to you.\n",
    "\n",
    "Simply open where.html in a browser to see the locations. You can hover over each map pin to find the location that the geocoding API returned for the user-entered input. If you cannot see any data when you open the where.html file, you might want to check the JavaScript or developer console for your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "# Visualizing networks and interconnections\n",
    "\n",
    "In this application, we will perform some of the functions of a search engine. We will first spider a small subset of the web and run a simplified version of the Google page rank algorithm to determine which pages are most highly connected, and then visualize the page rank and connectivity of our small corner of the web. We will use the D3 JavaScript visualization library http://d3js.org/ to produce the visualization output.\n",
    "\n",
    "You can download and extract this application from:\n",
    "\n",
    "pagerank.zip\n",
    "\n",
    "![](../images/pagerank.png)\n",
    "\n",
    "The first program (spider.py) program crawls a web site and pulls a series of pages into the database (spider.sqlite), recording the links between pages. You can restart the process at any time by removing the spider.sqlite file and rerunning spider.py.\n",
    "\n",
    "```\n",
    "Enter web url or enter: http://www.dr-chuck.com/\n",
    "['http://www.dr-chuck.com']\n",
    "How many pages:2\n",
    "1 http://www.dr-chuck.com/ 12\n",
    "2 http://www.dr-chuck.com/csev-blog/ 57\n",
    "How many pages:\n",
    "```\n",
    "\n",
    "\n",
    "In this sample run, we told it to crawl a website and retrieve two pages. If you restart the program and tell it to crawl more pages, it will not re-crawl any pages already in the database. Upon restart it goes to a random non-crawled page and starts there. So each successive run of spider.py is additive.\n",
    "\n",
    "```\n",
    "Enter web url or enter: http://www.dr-chuck.com/\n",
    "['http://www.dr-chuck.com']\n",
    "How many pages:3\n",
    "3 http://www.dr-chuck.com/csev-blog 57\n",
    "4 http://www.dr-chuck.com/dr-chuck/resume/speaking.htm 1\n",
    "5 http://www.dr-chuck.com/dr-chuck/resume/index.htm 13\n",
    "How many pages:\n",
    "```\n",
    "\n",
    "You can have multiple starting points in the same databaseâ€”within the program, these are called \"webs\". The spider chooses randomly amongst all non-visited links across all the webs as the next page to spider.\n",
    "\n",
    "If you want to dump the contents of the spider.sqlite file, you can run spdump.py as follows:\n",
    "\n",
    "\n",
    "```\n",
    "(5, None, 1.0, 3, 'http://www.dr-chuck.com/csev-blog')\n",
    "(3, None, 1.0, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')\n",
    "(1, None, 1.0, 2, 'http://www.dr-chuck.com/csev-blog/')\n",
    "(1, None, 1.0, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')\n",
    "4 rows.\n",
    "```\n",
    "\n",
    "This shows the number of incoming links, the old page rank, the new page rank, the id of the page, and the url of the page. The spdump.py program only shows pages that have at least one incoming link to them.\n",
    "\n",
    "Once you have a few pages in the database, you can run page rank on the pages using the sprank.py program. You simply tell it how many page rank iterations to run.\n",
    "\n",
    "```\n",
    "How many iterations:2\n",
    "1 0.546848992536\n",
    "2 0.226714939664\n",
    "[(1, 0.559), (2, 0.659), (3, 0.985), (4, 2.135), (5, 0.659)]\n",
    "```\n",
    "\n",
    "You can dump the database again to see that page rank has been updated:\n",
    "\n",
    "```\n",
    "(5, 1.0, 0.985, 3, 'http://www.dr-chuck.com/csev-blog')\n",
    "(3, 1.0, 2.135, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')\n",
    "(1, 1.0, 0.659, 2, 'http://www.dr-chuck.com/csev-blog/')\n",
    "(1, 1.0, 0.659, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')\n",
    "4 rows.\n",
    "```\n",
    "\n",
    "You can run sprank.py as many times as you like and it will simply refine the page rank each time you run it. You can even run sprank.py a few times and then go spider a few more pages sith spider.py and then run sprank.py to reconverge the page rank values. A search engine usually runs both the crawling and ranking programs all the time.\n",
    "\n",
    "If you want to restart the page rank calculations without respidering the web pages, you can use spreset.py and then restart sprank.py.\n",
    "\n",
    "```\n",
    "How many iterations:50\n",
    "1 0.546848992536\n",
    "2 0.226714939664\n",
    "3 0.0659516187242\n",
    "4 0.0244199333\n",
    "5 0.0102096489546\n",
    "6 0.00610244329379\n",
    "...\n",
    "42 0.000109076928206\n",
    "43 9.91987599002e-05\n",
    "44 9.02151706798e-05\n",
    "45 8.20451504471e-05\n",
    "46 7.46150183837e-05\n",
    "47 6.7857770908e-05\n",
    "48 6.17124694224e-05\n",
    "49 5.61236959327e-05\n",
    "50 5.10410499467e-05\n",
    "[(512, 0.0296), (1, 12.79), (2, 28.93), (3, 6.808), (4, 13.46)]\n",
    "```\n",
    "\n",
    "For each iteration of the page rank algorithm it prints the average change in page rank per page. The network initially is quite unbalanced and so the individual page rank values change wildly between iterations. But in a few short iterations, the page rank converges. You should run sprank.py long enough that the page rank values converge.\n",
    "\n",
    "If you want to visualize the current top pages in terms of page rank, run spjson.py to read the database and write the data for the most highly linked pages in JSON format to be viewed in a web browser.\n",
    "\n",
    "```\n",
    "Creating JSON output on spider.json...\n",
    "How many nodes? 30\n",
    "Open force.html in a browser to view the visualization\n",
    "```\n",
    "\n",
    "You can view this data by opening the file force.html in your web browser. This shows an automatic layout of the nodes and links. You can click and drag any node and you can also double-click on a node to find the URL that is represented by the node.\n",
    "\n",
    "If you rerun the other utilities, rerun spjson.py and press refresh in the browser to get the new data from spider.json.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
