{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving web pages with **`urllib`**\n",
    "\n",
    "While we can manually send and receive data over HTTP using the socket library, there is a much simpler way to perform this common task in Python by using the **urllib** library.\n",
    "\n",
    "Using **urllib**, you can treat a web page much like a file. You simply indicate which web page you would like to retrieve and **urllib** handles all of the HTTP protocol and header details.\n",
    "\n",
    "The equivalent code to read the https://docs.python.org/3/library/ file from the web using urllib is as follows:\n",
    "\n",
    "```python\n",
    "import urllib.request\n",
    "\n",
    "fhand = urllib.request.urlopen('https://docs.python.org/3/library/')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "```    \n",
    "\n",
    "Once the web page has been opened with **urllib.urlopen**, we can treat it like a file and read through it using a **for** loop.\n",
    "\n",
    "When the program runs, we only see the output of the contents of the file. The headers are still sent, but the **urllib** code consumes the headers and only returns the data to us.\n",
    "\n",
    "\n",
    "As an example, we can write a program to retrieve the data and compute the frequency of each word in the file as follows:\n",
    "\n",
    "```python\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('https://docs.python.org/3/library/')\n",
    "\n",
    "counts = dict()\n",
    "for line in fhand:\n",
    "    words = line.decode().split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "print(counts)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML and scraping the web\n",
    "\n",
    "One of the common uses of the **urllib** capability in Python is to scrape the web. Web scraping is when we write a program that pretends to be a web browser and retrieves pages, then examines the data in those pages looking for patterns.\n",
    "\n",
    "As an example, a search engine such as Google will look at the source of one web page and extract the links to other pages and retrieve those pages, extracting links, and so on. Using this technique, Google spiders its way through nearly all of the pages on the web.\n",
    "\n",
    "Google also uses the frequency of links from pages it finds to a particular page as one measure of how \"important\" a page is and how high the page should appear in its search results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "# Parsing HTML using regular expressions\n",
    "\n",
    "One simple way to parse HTML is to use regular expressions to repeatedly search for and extract substrings that match a particular pattern.\n",
    "\n",
    "Here is a simple web page:\n",
    "\n",
    "```python\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "We can construct a well-formed regular expression to match and extract the link values from the above text as follows:\n",
    "\n",
    "```python\n",
    "href=\"http[s]?://.+?\"\n",
    "```\n",
    "\n",
    "Our regular expression looks for strings that start with \"href=\"http://\" or \"href=\"https://\", followed by one or more characters **`(.+?)`**, followed by another double quote. The question mark behind the **`[s]?`** indicates to search for the string \"http\" followed by zero or one \"s\".\n",
    "\n",
    "The question mark added to the **`.+?`** indicates that the match is to be done in a \"non-greedy\" fashion instead of a \"greedy\" fashion. A non-greedy match tries to find the smallest possible matching string and a greedy match tries to find the largest possible matching string.\n",
    "\n",
    "We add parentheses to our regular expression to indicate which part of our matched string we would like to extract, and produce the following program:\n",
    "\n",
    "```python\n",
    "# Search for link values within URL input\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "#url = input('Enter - ')\n",
    "url = \"https://docs.python.org\"\n",
    "html = urllib.request.urlopen(url).read()\n",
    "links = re.findall(b'href=\"(http[s]?://.*?)\"', html)\n",
    "for link in links:\n",
    "    print(link.decode())\n",
    "```    \n",
    "\n",
    "The **ssl** library allows this program to access web sites that strictly enforce HTTPS. The **read** method returns HTML source code as a bytes object instead of returning an HTTPResponse object. The **findall** regular expression method will give us a list of all of the strings that match our regular expression, returning only the link text between the double quotes.\n",
    "\n",
    "When we run the program and input a URL, we get the following output:\n",
    "\n",
    "```\n",
    "Enter - https://docs.python.org\n",
    "https://docs.python.org/3/index.html\n",
    "https://www.python.org/\n",
    "https://docs.python.org/3.8/\n",
    "https://docs.python.org/3.7/\n",
    "https://docs.python.org/3.5/\n",
    "https://docs.python.org/2.7/\n",
    "https://www.python.org/doc/versions/\n",
    "https://www.python.org/dev/peps/\n",
    "https://wiki.python.org/moin/BeginnersGuide\n",
    "https://wiki.python.org/moin/PythonBooks\n",
    "https://www.python.org/doc/av/\n",
    "https://www.python.org/\n",
    "https://www.python.org/psf/donations/\n",
    "http://sphinx.pocoo.org/\n",
    "```\n",
    "\n",
    "Regular expressions work very nicely when your HTML is well formatted and predictable. But since there are a lot of \"broken\" HTML pages out there, a solution only using regular expressions might either miss some valid links or end up with bad data.\n",
    "\n",
    "This can be solved by using a robust HTML parsing library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "# Parsing HTML using BeautifulSoup\n",
    "\n",
    "\n",
    "Even though HTML looks like XML1 and some pages are carefully constructed to be XML, most HTML is generally broken in ways that cause an XML parser to reject the entire page of HTML as improperly formed.\n",
    "\n",
    "There are a number of Python libraries which can help you parse HTML and extract data from the pages. Each of the libraries has its strengths and weaknesses and you can pick one based on your needs.\n",
    "\n",
    "As an example, we will simply parse some HTML input and extract links using the BeautifulSoup library. BeautifulSoup tolerates highly flawed HTML and still lets you easily extract the data you need. You can download and install the BeautifulSoup code from:\n",
    "\n",
    "[https://pypi.python.org/pypi/beautifulsoup4](https://pypi.python.org/pypi/beautifulsoup4)\n",
    "\n",
    "Information on installing BeautifulSoup with the Python Package Index tool **pip** is available at:\n",
    "\n",
    "[https://packaging.python.org/tutorials/installing-packages/](https://packaging.python.org/tutorials/installing-packages/)\n",
    "\n",
    "We will use **urllib** to read the page and then use **BeautifulSoup** to extract the **href** attributes from the anchor (**a**) tags.\n",
    "\n",
    "```python\n",
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - https://docs.python.org\n",
      "genindex.html\n",
      "py-modindex.html\n",
      "https://www.python.org/\n",
      "#\n",
      "whatsnew/3.7.html\n",
      "whatsnew/index.html\n",
      "tutorial/index.html\n",
      "library/index.html\n",
      "reference/index.html\n",
      "using/index.html\n",
      "howto/index.html\n",
      "installing/index.html\n",
      "distributing/index.html\n",
      "extending/index.html\n",
      "c-api/index.html\n",
      "faq/index.html\n",
      "py-modindex.html\n",
      "genindex.html\n",
      "glossary.html\n",
      "search.html\n",
      "contents.html\n",
      "bugs.html\n",
      "about.html\n",
      "license.html\n",
      "copyright.html\n",
      "download.html\n",
      "https://docs.python.org/3.9/\n",
      "https://docs.python.org/3.8/\n",
      "https://docs.python.org/3.7/\n",
      "https://docs.python.org/3.6/\n",
      "https://docs.python.org/3.5/\n",
      "https://docs.python.org/2.7/\n",
      "https://www.python.org/doc/versions/\n",
      "https://www.python.org/dev/peps/\n",
      "https://wiki.python.org/moin/BeginnersGuide\n",
      "https://wiki.python.org/moin/PythonBooks\n",
      "https://www.python.org/doc/av/\n",
      "genindex.html\n",
      "py-modindex.html\n",
      "https://www.python.org/\n",
      "#\n",
      "copyright.html\n",
      "https://www.python.org/psf/donations/\n",
      "bugs.html\n",
      "http://sphinx.pocoo.org/\n"
     ]
    }
   ],
   "source": [
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "#url = input('Enter - ')\n",
    "url = \"https://docs.python.org\"\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
